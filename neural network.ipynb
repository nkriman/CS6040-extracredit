{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Basics\n",
    "\n",
    "## Neural Network Architecture: \n",
    "Understand the basic components, such as input layer, hidden layers, output layer, neurons, weights, and biases.\n",
    "## Activation Functions: \n",
    "Learn about functions like Sigmoid, ReLU, and Softmax.\n",
    "## Loss Functions: \n",
    "Understand concepts of loss functions like Mean Squared Error (MSE) or Cross-Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Outline of a Neural Network Project\n",
    "\n",
    "## Introduction\n",
    "- **Objective**: Design and implement a neural network using only NumPy, focusing on the fundamental principles of machine learning and linear algebra.\n",
    "\n",
    "## 1. Neural Network Architecture\n",
    "- **Linear Algebra Perspective**: View neurons and layers as vectors and matrices.\n",
    "- **Layers**: Input, hidden, and output layers, each represented by matrices of weights and bias vectors.\n",
    "- **Neurons**: Units of computation, aggregating input through matrix operations.\n",
    "\n",
    "## 2. Mathematical Foundations\n",
    "- **Matrix Operations**: Emphasize the use of matrix multiplication, addition, and transpose operations in the network's computations.\n",
    "- **Activation Functions**: Conceptualize as vectorized non-linear transformations (e.g., Sigmoid or ReLU applied element-wise).\n",
    "\n",
    "## 3. Forward Propagation\n",
    "- **Linear Transformation**: `Z = W*X + b`, where `W` and `X` are matrices, and `b` is a bias vector.\n",
    "- **Activation**: Apply an activation function on `Z` to introduce non-linearity.\n",
    "\n",
    "## 4. Loss Function\n",
    "- **Theory**: Understand the purpose of a loss function in quantifying prediction error.\n",
    "- **Implementation**: Use a function like MSE or Cross-Entropy, represented in a linear algebra framework.\n",
    "\n",
    "## 5. Backward Propagation\n",
    "- **Gradient Computation**: Utilize the chain rule in a matrix calculus context to compute gradients of the loss with respect to weights and biases.\n",
    "- **Backpropagation Algorithm**: Conceptually, this is the reverse of forward propagation, applying the chain rule through layers.\n",
    "\n",
    "## 6. Parameter Update\n",
    "- **Gradient Descent**: Update the weights and biases by moving in the direction opposite to the gradient.\n",
    "- **Learning Rate**: Control the step size in the gradient descent.\n",
    "\n",
    "## 7. Model Integration and Training\n",
    "- **Epochs**: Iterate over the dataset multiple times.\n",
    "- **Batch Processing**: Optionally introduce the concept of mini-batch gradient descent.\n",
    "\n",
    "## 8. Evaluation and Prediction\n",
    "- **Predictive Function**: Apply the trained model to new data.\n",
    "- **Performance Metrics**: Evaluate accuracy, precision, recall, etc.\n",
    "\n",
    "## 9. Experimental Setup\n",
    "- **Dataset**: Choose an appropriate dataset for testing.\n",
    "- **Training/Testing Split**: Emphasize the importance of unbiased model evaluation.\n",
    "\n",
    "## 10. Advanced Topics (Optional)\n",
    "- **Regularization**: Techniques like L1, L2 regularization to prevent overfitting.\n",
    "- **Optimization Algorithms**: Beyond basic gradient descent, explore algorithms like Adam, RMSprop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Single Layer Neural Networks from a Statistical Perspective\n",
    "(based on \"Introduction to statistical learning\")\n",
    "\n",
    "## Conceptual Overview\n",
    "- **Purpose**: A neural network aims to build a nonlinear function `f(X)` to predict the response `Y`, using an input vector of variables `X = (X1, X2, ..., Xp)`.\n",
    "- **Distinction**: Unlike trees, boosting, and generalized additive models, neural networks have a unique structure characterized by layers and units.\n",
    "\n",
    "## Neural Network Model\n",
    "- **Input Layer**: Consists of features `X1, ..., Xp` as units.\n",
    "- **Hidden Layer**: Each input feeds into `K` hidden units (in this example, `K=5`).\n",
    "- **Model Representation**:\n",
    "  $$ f(X) = β0 + ∑_{k=1}^{K} βk g(wk0 + ∑_{j=1}^{p} wkj Xj) $$\n",
    "- **Activation Functions**: `g(z)` is a nonlinear function. Popular choices are sigmoid and ReLU (Rectified Linear Unit).\n",
    "\n",
    "## Activation Functions\n",
    "- **Sigmoid**: \n",
    "  $ g(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **ReLU**:\n",
    "  $ g(z) = max(0, z) $\n",
    "- **Efficiency**: ReLU is preferred for its computational efficiency.\n",
    "\n",
    "## Computation in Hidden Layer\n",
    "- **Activations**: Calculated as $Ak = hk(X) = g(wk0 + ∑_{j=1}^{p} wkj Xj)$.\n",
    "- **Role of Activations**: Similar to basis functions, transforming original features.\n",
    "\n",
    "## Output Layer\n",
    "- **Formulation**: The output is a linear regression in the activations `Ak`.\n",
    "- **Parameters**: Includes both weights `w` and biases `β`, to be estimated from data.\n",
    "\n",
    "## Nonlinearity and Interaction Effects\n",
    "- Nonlinear activation functions allow the model to capture complex patterns and interactions.\n",
    "- Example with quadratic `g(z)` illustrates how nonlinear transformations can model interactions.\n",
    "\n",
    "## Fitting the Neural Network\n",
    "- **Loss Function**: Typically squared-error loss for quantitative responses.\n",
    "- **Optimization**: Minimize the sum of squared errors between predictions and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section is based on the paper \"The Neural Network, its Techniques and Applications\" by Casey Schafer (2016)*\n",
    "\n",
    "# Basic Neural Network: Perceptron Explanation\n",
    "\n",
    "A **perceptron** is the most basic form of a neural network. It consists of one layer of inputs (independent variables or features) and one layer of outputs (dependent variables). Let's break down the components and the operation of this network step by step, exemplifying with :\n",
    "\n",
    "## Layers and Vectors\n",
    "\n",
    "1. **Layers**: In a perceptron, layers are visualized as a series of nodes (each node corresponding to a variable) aligned vertically.\n",
    "   - An **input layer** is represented by a vector for one observation. For example, vector `x` is `[x1 x2 ... xn]ᵀ` for \"n\" features.\n",
    "   - With k observations and n features, a layer is represented as a matrix `n × k` .\n",
    "\n",
    "2. **Input Layer (X)**: For a perceptron with two features and `k` observations, the input layer is a `2 × k` matrix (matrix `X`). We are going to model the \"mini MNIST\" data, which will have (28 x 28) input features and 1.000 observations, so the input layer matrix has a 784 x 1.000 shape.\n",
    "\n",
    "3. **Output Layer (Y)**: \n",
    "   - The output layer (matrix `Y`) is typically `n × p` in this case we will model each possible digit as a binary variable, so it will have 10 x 1.000 shape.\n",
    "   - `Y` is the predicted output by the network, different from `T` (label matrix), which is the known output in supervised learning.\n",
    "\n",
    "## Goal of the Neural Network\n",
    "\n",
    "- The objective is to minimize the difference between the known output `T` and the predicted output `Y`.\n",
    "- The ideal network predicts `T` accurately using only the inputs.\n",
    "\n",
    "## Weight Matrix (W)\n",
    "\n",
    "- To map from `X` to `Y`, we introduce a weight matrix `W`.\n",
    "- In general, `W` is an `n × m` matrix, where `m` is the number of input nodes, independent of the number of observations. In this case we have 784 input features and 10 output features, so it has a 10 x 784 shape.\n",
    "- The fundamental equation of the network is: `T = W * X + b`, where `b` is a bias vector.\n",
    "\n",
    "## Linearization\n",
    "\n",
    "- To linearize this equation, augment `W` with `b` as `[W | b]` and add a row of 1’s to `X`.\n",
    "- The equation becomes: `T = [W | b] * [X; 1]`.\n",
    "- We redefine `W` to be `n × (m + 1)` and `X` to be `(m + 1) × p` with a row of 1’s at the bottom.\n",
    "\n",
    "## Solving for W\n",
    "\n",
    "- We don't \"solve\" this equation in the traditional sense. Instead, we compute the pseudoinverse of `X` to find an approximation of `W`, denoted as `Ŵ`.\n",
    "- `Ŵ` is calculated as `Ŵ = T * V * Σ⁻¹ * Uᵀ`, where `V`, `Σ`, and `U` are derived from the singular value decomposition of `X`.\n",
    "- This `Ŵ` is a projection onto the column space of `X`.\n",
    "\n",
    "## Predicted Output (Y)\n",
    "\n",
    "- The predicted output `Y` is computed as `Y = Ŵ * X`.\n",
    "- `Y` equals `T` only if `T` is in the column space of `X`, which is rare.\n",
    "- Otherwise, `Y` is as close to `T` as possible, given the constraints.\n",
    "\n",
    "## Limitations and Extensions\n",
    "\n",
    "- This linear algebra method works best if the relationship between `X` and `T` is linear, which is also rare.\n",
    "- For more complex relationships, we extend the model to a more sophisticated neural network that can handle non-linear functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a subset of the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "images = data['images'] # a list of 1000 numpy image matrices\n",
    "labels = data['labels'] # a list of 1000 integer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (28, 28)\n",
      "Flattened image shape:  (784,)\n",
      "X shape: (784, 1000)\n",
      "T shape: (10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original image shape:\",images[0].shape) # (28, 28)\n",
    "\n",
    "# Given we are working with a simple perceptron, we must flatten each image into a vector of length 784.\n",
    "\n",
    "# Example:\n",
    "\n",
    "print(\"Flattened image shape: \",images[0].flatten().shape)\n",
    "\n",
    "# Now we can create a single matrix of all the images, where each row is an image vector.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([image.flatten() for image in images])\n",
    "X = X.T # transpose so that each column is an image vector\n",
    "print(\"X shape:\",X.shape) # (784,1000)\n",
    "\n",
    "# We also need to convert the labels so that they are one-hot encoded.\n",
    "# Given that each label is an integer between 0 and 9, we can create a 10-dimensional vector for each label.\n",
    "# The vector will have a 1 in the position of the label and 0s everywhere else.\n",
    "\n",
    "T = np.zeros((len(labels), 10))\n",
    "for i, label in enumerate(labels):\n",
    "    T[i, label] = 1\n",
    "T = T.T # transpose so that each column is a label vector\n",
    "print(\"T shape:\",T.shape) # (10,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 784)\n",
      "T shape: (10, 1000)\n",
      "X: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "T: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 1.]]\n",
      "W_hat shape: (10, 784)\n",
      "W_hat: [[ 2.11237658e+09 -1.04955834e+11 -5.13363801e+10 ...  3.85621959e+10\n",
      "  -5.46846281e+10  9.56947394e+09]\n",
      " [-3.30079618e+10  1.10669870e+10 -2.14677754e+10 ...  2.03558402e+10\n",
      "   3.91054613e+10  2.90604952e+10]\n",
      " [ 3.11741740e+10 -4.23477634e+10  7.80977818e+08 ... -1.43691528e+10\n",
      "  -4.59679423e+10  5.12355510e+10]\n",
      " ...\n",
      " [ 6.33463940e+10 -1.11043005e+10 -2.16872418e+10 ...  5.43620773e+10\n",
      "   6.17301138e+10  6.56674428e+10]\n",
      " [ 8.62112372e+10 -1.36373543e+10  6.63989055e+09 ...  4.41542226e+10\n",
      "   7.58351793e+10  1.72868239e+10]\n",
      " [-1.63810193e+11  2.17931590e+10  3.70171170e+10 ... -9.68784683e+10\n",
      "  -1.22973266e+11 -8.70428261e+10]]\n",
      "New X shape: (784, 1000)\n",
      "New X: [[-1.04644165  0.17006635 -0.63005947 ...  0.67462301  0.80101034\n",
      "  -0.50457856]\n",
      " [ 0.56490669  0.73194209  0.03372531 ... -0.70613006  0.64201168\n",
      "   0.6672048 ]\n",
      " [-0.61496876  0.21970827  0.09071214 ... -0.01115468  0.12773824\n",
      "  -0.1279802 ]\n",
      " ...\n",
      " [ 1.09910928  0.88809108 -0.49811356 ...  0.61048987  1.27203942\n",
      "  -0.22901043]\n",
      " [-0.33379694 -0.03252611  0.63741908 ... -1.61097658 -0.39490506\n",
      "   0.37866906]\n",
      " [ 0.07536334 -0.20508583  0.51657924 ... -2.18079835  0.11410507\n",
      "   0.01825243]]\n",
      "Predictions: [[ 3.76141504e+11 -7.39334640e+11 -2.90371566e+11 ... -5.32070405e+11\n",
      "   3.13599903e+10 -7.49112010e+11]\n",
      " [-3.80369456e+11 -9.73817564e+11  3.77048770e+11 ...  7.98050452e+11\n",
      "  -2.16059883e+11 -4.88507035e+10]\n",
      " [ 2.94489090e+11  9.79390102e+11 -4.94020292e+11 ... -1.29645298e+11\n",
      "  -3.73508425e+11 -2.27018434e+11]\n",
      " ...\n",
      " [-9.64966435e+10  4.82917537e+10 -1.07063861e+12 ...  1.02258155e+12\n",
      "   7.33633276e+10  5.15703793e+11]\n",
      " [-2.05638086e+11  2.52900465e+11  8.98115787e+11 ... -1.34172913e+12\n",
      "   2.31240421e+12 -7.12401340e+10]\n",
      " [ 1.17379707e+12 -1.77677555e+11  1.42158079e+12 ... -1.24990223e+12\n",
      "  -1.00775601e+12 -3.08027266e+11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_weights(n_inputs, n_outputs):\n",
    "    \"\"\"\n",
    "    Randomly initializes the weight matrix and bias vector.\n",
    "    \"\"\"\n",
    "    W = np.random.randn(n_outputs, n_inputs)\n",
    "    b = np.random.randn(n_outputs, 1)\n",
    "    return W, b\n",
    "\n",
    "def linear_transform(X, W, b):\n",
    "    \"\"\"\n",
    "    Computes the linear transformation W * X + b.\n",
    "    \"\"\"\n",
    "    return np.dot(W, X) + b\n",
    "\n",
    "def compute_pseudoinverse(X):\n",
    "    \"\"\"\n",
    "    Computes the pseudoinverse of matrix X.\n",
    "    \"\"\"\n",
    "    U, S, VT = np.linalg.svd(X, full_matrices=False)\n",
    "    S_inv = np.diag(1 / S)\n",
    "    X_pinv = np.dot(VT.T, np.dot(S_inv, U.T))\n",
    "    return X_pinv\n",
    "\n",
    "def train_perceptron(X, T):\n",
    "    \"\"\"\n",
    "    Trains the perceptron using pseudoinverse to find the optimal weights.\n",
    "    \"\"\"\n",
    "    X_pinv = compute_pseudoinverse(X)\n",
    "    W_hat = np.dot(T, X_pinv)\n",
    "    return W_hat\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Uses the trained perceptron to predict outputs.\n",
    "    \"\"\"\n",
    "    return np.dot(W, X)\n",
    "\n",
    "# Example usage\n",
    "n_inputs = 784  # Number of input nodes\n",
    "n_outputs = 10 # Number of output nodes\n",
    "p = 1000         # Number of observations\n",
    "\n",
    "# Random example data\n",
    "#X = np.random.randn(n_inputs, p)  # Input data\n",
    "#T = np.random.randn(n_outputs, p) # Target output\n",
    "\n",
    "print(\"X shape:\", X.T.shape)\n",
    "print(\"T shape:\", T.shape)\n",
    "print(\"X:\", X)\n",
    "print(\"T:\", T)\n",
    "\n",
    "# Training the perceptron\n",
    "W_hat = train_perceptron(X, T)\n",
    "print(\"W_hat shape:\", W_hat.shape)\n",
    "print(\"W_hat:\", W_hat)\n",
    "\n",
    "# Predicting new inputs\n",
    "new_X = np.random.randn(n_inputs, p)\n",
    "\n",
    "print(\"New X shape:\", new_X.shape)\n",
    "print(\"New X:\", new_X)\n",
    "\n",
    "\n",
    "predictions = predict(new_X, W_hat)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
