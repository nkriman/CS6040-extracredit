{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Single Layer Neural Networks from a Statistical Perspective\n",
    "(based on \"Introduction to statistical learning\")\n",
    "\n",
    "## Conceptual Overview\n",
    "- **Purpose**: A neural network aims to build a nonlinear function `f(X)` to predict the response `Y`, using an input vector of variables `X = (X1, X2, ..., Xp)`.\n",
    "- **Distinction**: Unlike trees, boosting, and generalized additive models, neural networks have a unique structure characterized by layers and units.\n",
    "\n",
    "## Neural Network Model\n",
    "- **Input Layer**: Consists of features `X1, ..., Xp` as units.\n",
    "- **Hidden Layer**: Each input feeds into `K` hidden units (in this example, `K=5`).\n",
    "- **Model Representation**:\n",
    "  $$ f(X) = β0 + ∑_{k=1}^{K} βk g(wk0 + ∑_{j=1}^{p} wkj Xj) $$\n",
    "- **Activation Functions**: `g(z)` is a nonlinear function. Popular choices are sigmoid and ReLU (Rectified Linear Unit).\n",
    "\n",
    "## Activation Functions\n",
    "- **Sigmoid**: \n",
    "  $ g(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **ReLU**:\n",
    "  $ g(z) = max(0, z) $\n",
    "- **Efficiency**: ReLU is preferred for its computational efficiency.\n",
    "\n",
    "## Computation in Hidden Layer\n",
    "- **Activations**: Calculated as $Ak = hk(X) = g(wk0 + ∑_{j=1}^{p} wkj Xj)$.\n",
    "- **Role of Activations**: Similar to basis functions, transforming original features.\n",
    "\n",
    "## Output Layer\n",
    "- **Formulation**: The output is a linear regression in the activations `Ak`.\n",
    "- **Parameters**: Includes both weights `w` and biases `β`, to be estimated from data.\n",
    "\n",
    "## Nonlinearity and Interaction Effects\n",
    "- Nonlinear activation functions allow the model to capture complex patterns and interactions.\n",
    "- Example with quadratic `g(z)` illustrates how nonlinear transformations can model interactions.\n",
    "\n",
    "## Fitting the Neural Network\n",
    "- **Loss Function**: Typically squared-error loss for quantitative responses.\n",
    "- **Optimization**: Minimize the sum of squared errors between predictions and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section is based on the paper \"The Neural Network, its Techniques and Applications\" by Casey Schafer (2016)*\n",
    "\n",
    "# Basic Neural Network: Perceptron\n",
    "\n",
    "A **perceptron** is the most basic form of a neural network. It consists of one layer of inputs (independent variables or features) and one layer of outputs (dependent variables). Let's break down the components and the operation of this network step by step, exemplifying with :\n",
    "\n",
    "## Layers and Vectors\n",
    "\n",
    "1. **Layers**: In a perceptron, layers are visualized as a series of nodes (each node corresponding to a variable) aligned vertically.\n",
    "   - An **input layer** is represented by a vector for one observation. For example, vector `x` is `[x1 x2 ... xn]ᵀ` for \"n\" features.\n",
    "   - With k observations and n features, a layer is represented as a matrix `n × k` .\n",
    "\n",
    "2. **Input Layer (X)**: For a perceptron with two features and `k` observations, the input layer is a `2 × k` matrix (matrix `X`). We are going to model the \"mini MNIST\" data, which will have (28 x 28) input features and 1.000 observations, so the input layer matrix has a 784 x 1.000 shape.\n",
    "\n",
    "3. **Output Layer (Y)**: \n",
    "   - The output layer (matrix `Y`) is typically `n × p` in this case we will model each possible digit as a binary variable, so it will have 10 x 1.000 shape.\n",
    "   - `Y` is the predicted output by the network, different from `T` (label matrix), which is the known output in supervised learning.\n",
    "\n",
    "## Goal of the Neural Network\n",
    "\n",
    "- The objective is to minimize the difference between the known output `T` and the predicted output `Y`.\n",
    "- The ideal network predicts `T` accurately using only the inputs.\n",
    "\n",
    "## Weight Matrix (W)\n",
    "\n",
    "- To map from `X` to `Y`, we introduce a weight matrix `W`.\n",
    "- In general, `W` is an `n × m` matrix, where `m` is the number of input nodes, independent of the number of observations. In this case we have 784 input features and 10 output features, so it has a 10 x 784 shape.\n",
    "- The fundamental equation of the network is: `T = W * X + b`, where `b` is a bias vector.\n",
    "\n",
    "## Linearization\n",
    "\n",
    "- To linearize this equation, augment (append as a last column) `W` with `b` as `[W | b]` and add a row of 1’s at the end of matrix `X`, which will be the matrix equivalent of $W*X + b$.\n",
    "- The equation becomes: `T = [W | b] * [X; 1]`.\n",
    "- We must redefine the shape of matrix `W` to be `n × (m + 1)` and also change the shape of matrix `X` to be `(m + 1) × p` with a row of 1’s at the bottom.\n",
    "- We now need to solve $T = WX$\n",
    "\n",
    "## Solving for W\n",
    "\n",
    "- We don't \"solve\" this equation in the traditional sense. Instead, we compute the pseudoinverse of `X` using singular value decomposition (SVD) to find an approximation of `W`, denoted as `Ŵ`.\n",
    "- We use the \"pseudoinverse\" instead of the \"inverse\" of a matrix ($A^-1$) because the pseudoinverse can be used in non-square and in non-invertible matrixes (which is incidentally our case for the MNIST dataset)\n",
    "- We remember from one of our beloved past notebooks in CS6040 that any matrix A can be expressed as $A = U \\Sigma V^T$\n",
    "- Now the approximation `Ŵ` is calculated as `Ŵ = T * V * Σ⁻¹ * Uᵀ`, where `V`, `Σ`, and `U` are derived from the SVD of `X`.\n",
    "- This `Ŵ` is a projection onto the column space of `X`.\n",
    "\n",
    "## Predicted Output (Y)\n",
    "\n",
    "- The predicted output `Y` is computed as `Y = Ŵ * X`.\n",
    "\n",
    "\n",
    "## Limitations and Extensions\n",
    "\n",
    "- This linear algebra method works best if the relationship between `X` and `T` is linear, which is also rare.\n",
    "- For more complex relationships, we extend the model to a more sophisticated neural network that can handle non-linear functions.\n",
    "\n",
    "# First implementation\n",
    "\n",
    "Before going into the more complex model, let's apply what we have defined so far.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-MNIST Dataset loading\n",
    "\n",
    "We will be working with a subset of the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is a 2 digit\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "images = data['images'] # a list of 1000 numpy image matrices\n",
    "labels = data['labels'] # a list of 1000 integer labels\n",
    "\n",
    "# Lets visualize an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(images[234], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"This image is a\", labels[234], \"digit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we are working with a simple perceptron, we must flatten each image into a vector of length 784. After we have done that, we can create a single matrix of all the images, where each row is an image vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (28, 28)\n",
      "Flattened image shape:  (784,)\n",
      "X shape: (784, 1000)\n",
      "T shape: (10, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"Original image shape:\",images[0].shape) # (28, 28)\n",
    "\n",
    "print(\"Flattened image shape: \",images[0].flatten().shape)\n",
    "\n",
    "X = np.array([image.flatten() for image in images])\n",
    "X = X.T # transpose so that each column is an image vector\n",
    "print(\"X shape:\",X.shape) # (784,1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert the labels so that they are one-hot encoded.\n",
    "Given that each label is an integer between 0 and 9, we can create a 10-dimensional vector for each label row.\n",
    "The vector will have a 1 in the position of the digit label and 0s everywhere else (a very *sparse* matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shape: (10, 1000)\n"
     ]
    }
   ],
   "source": [
    "T = np.zeros((len(labels), 10))\n",
    "for i, label in enumerate(labels):\n",
    "    T[i, label] = 1\n",
    "T = T.T # transpose so that each column is a label vector\n",
    "print(\"T shape:\",T.shape) # (10,1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start coding, let's make sure we separate our dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (784, 800)\n",
      "X_test shape: (784, 200)\n",
      "T_train shape: (10, 800)\n",
      "T_test shape: (10, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, T_train, T_test = train_test_split(X.T, T.T, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "T_train = T_train.T\n",
    "T_test = T_test.T\n",
    "\n",
    "print(\"X_train shape:\",X_train.shape) # (784,800)\n",
    "print(\"X_test shape:\",X_test.shape) # (784,200)\n",
    "print(\"T_train shape:\",T_train.shape) # (10,800)\n",
    "print(\"T_test shape:\",T_test.shape) # (10,200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding our first implementation\n",
    "\n",
    "We are now ready to start coding our simple perceptron implementation. For this very simple algorithm, we won't even need to intialize W and b , because we are estimating W directly from just the labels and X matrix (T_train and X_train, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (784, 800)\n",
      "T train shape: (10, 800)\n",
      "X: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "T: [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "W_hat shape: (10, 784)\n",
      "W_hat: [[ 0.00000000e+00  3.12886796e-12 -9.20112965e-11 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -2.95563456e-12  2.03927817e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -6.69842085e-12  2.03494454e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00 -1.45313511e-12  2.00398931e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  9.49960861e-12 -1.13510918e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  2.53287261e-12 -3.33712346e-10 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "New X shape: (784, 200)\n",
      "New X: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Predictions: [[ 1.00974013e-01 -2.00563329e-01 -1.14483738e+00 ... -7.86063327e-01\n",
      "   3.46629106e-01  4.87009576e-01]\n",
      " [-1.45053946e+00  1.06690899e+00  1.31835037e+00 ...  3.60711656e-02\n",
      "  -3.41053696e-01 -4.23716582e-01]\n",
      " [ 1.40587244e+00 -2.65668299e-02 -7.03071570e-01 ...  2.44607590e-01\n",
      "   1.30307437e-01  4.18171161e-01]\n",
      " ...\n",
      " [-5.90705091e-01  1.51746307e-01  9.42114123e-02 ...  5.46478461e-01\n",
      "  -3.92176996e-04  3.03919651e-01]\n",
      " [ 1.84343708e+00  1.45187195e-01  1.44228936e+00 ... -2.37569804e-01\n",
      "   1.56562072e-01  7.37636931e-02]\n",
      " [-2.46066370e-01 -1.12884995e-01 -1.56041056e+00 ... -8.20770287e-01\n",
      "   1.59192178e-01 -3.15347216e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_pseudoinverse(X):\n",
    "    \"\"\"\n",
    "    Computes the pseudoinverse of matrix X.\n",
    "    \"\"\"\n",
    "    X_pinv = np.linalg.pinv(X) # For simplicity let's just use numpy's pinv function\n",
    "    return X_pinv\n",
    "\n",
    "def train_perceptron(X, T):\n",
    "    \"\"\"\n",
    "    Trains the perceptron using pseudoinverse to find the optimal weights.\n",
    "    \"\"\"\n",
    "    X_pinv = compute_pseudoinverse(X)\n",
    "    W_hat = np.dot(T, X_pinv)\n",
    "    return W_hat\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Uses the trained perceptron to predict outputs.\n",
    "    \"\"\"\n",
    "    return np.dot(W, X)\n",
    "\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "print(\"T train shape:\", T_train.shape)\n",
    "print(\"X:\", X_train)\n",
    "print(\"T:\", T_train)\n",
    "\n",
    "# Training the perceptron\n",
    "W_hat = train_perceptron(X_train, T_train)\n",
    "print(\"W_hat shape:\", W_hat.shape)\n",
    "print(\"W_hat:\", W_hat)\n",
    "\n",
    "# Predicting new inputs\n",
    "new_X = X_test\n",
    "\n",
    "print(\"New X shape:\", new_X.shape)\n",
    "print(\"New X:\", new_X)\n",
    "\n",
    "\n",
    "predictions = predict(new_X, W_hat)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (10, 200)\n",
      "y_pred shape: (200,)\n",
      "y_pred: [3 1 8 8 4 0 6 5 3 1 8 0 6 9 9 8 2 5 7 7 0 1 3 5 8 3 1 1 8 1 3 1 6 4 7 1 6\n",
      " 5 6 0 5 0 3 9 9 9 9 6 4 9 3 8 2 5 5 0 6 0 9 8 8 0 5 2 6 6 6 8 9 5 1 9 5 7\n",
      " 3 4 6 0 5 5 8 0 7 0 3 1 1 4 1 4 8 4 5 1 7 4 4 0 0 5 5 1 4 1 6 6 1 9 1 9 1\n",
      " 8 9 8 5 4 1 9 1 5 0 4 9 6 9 3 6 8 8 8 8 5 2 9 3 0 1 1 0 9 0 8 1 4 8 9 3 5\n",
      " 6 3 4 1 5 4 9 8 4 6 0 1 8 1 6 3 8 0 0 6 3 3 3 9 1 9 4 3 2 9 8 6 4 5 3 5 0\n",
      " 1 3 3 9 7 9 8 6 1 2 4 3 6 6 0]\n",
      "y_true shape: (200,)\n",
      "y_true: [5 7 7 6 4 6 6 5 8 1 8 0 6 9 9 8 2 8 7 3 5 1 3 5 2 2 1 9 9 9 3 1 6 4 3 1 6\n",
      " 5 9 0 3 0 3 9 2 2 9 6 4 8 5 8 2 5 0 0 6 0 2 8 2 0 5 2 6 6 6 8 0 5 1 4 5 7\n",
      " 2 2 6 0 5 3 6 0 2 0 3 5 9 7 7 3 3 4 5 2 7 4 7 0 0 8 7 3 9 2 6 6 5 9 1 9 5\n",
      " 8 9 8 0 4 1 9 5 2 8 4 7 2 2 5 5 4 8 8 5 8 2 9 5 0 1 2 0 0 0 2 7 2 8 9 8 2\n",
      " 8 3 6 1 9 4 9 4 3 6 0 8 5 7 6 6 8 6 2 6 3 3 3 4 1 9 4 3 2 2 8 6 4 6 3 5 8\n",
      " 1 3 3 9 7 9 8 6 1 2 4 3 2 6 0]\n",
      "Accuracy: 0.59\n",
      "Precision: 0.6233987770298036\n",
      "Recall: 0.6088724994811952\n",
      "F1 score: 0.5751206504294888\n"
     ]
    }
   ],
   "source": [
    "# We can now calculate the prediction performance using sklearn performance metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# We need to convert the predictions from a matrix to a vector of labels.\n",
    "# We can do this by taking the index of the maximum value in each column.\n",
    "\n",
    "y_pred = np.argmax(predictions, axis=0)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"y_pred shape:\", y_pred.shape)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "# Now we can calculate the performance metrics.\n",
    "\n",
    "y_true = np.argmax(T_test, axis=0)\n",
    "print(\"y_true shape:\", y_true.shape)\n",
    "print(\"y_true:\", y_true)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred, average='macro'))\n",
    "print(\"F1 score:\", f1_score(y_true, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Amazing performance from this little perceptron guy! Let's move on to the theory again to introduce a new concept: \"hidden\" layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Hidden Layers\n",
    "\n",
    "We want to be able to model more complex relationships. We could assume that the relationship between the flattened matrix of an image and the digit represented in that image is *not* linear, and instead implies a complex relationship (although this is surprisingly controversial: https://stats.stackexchange.com/questions/426873/how-does-a-simple-logistic-regression-model-achieve-a-92-classification-accurac). To model that increased complexity we can use what is called a \"hidden layer\".\n",
    "\n",
    "## Enhanced Structure\n",
    "\n",
    "In between the input and the output layers, we will add a middle layer. This means that instead of going directly from a 784 dimensional space to a 10 dimensional one, we will go from 784 dimensions, to S dimensions, and from S dimensions to 10 dimensions. Our middle (hidden) layer then is an S dimensional space. For our case we will use 128 dimensions for our hidden layer.\n",
    "\n",
    "## Weight Matrix and Adaptive Learning\n",
    "\n",
    "Now for estimating our weight matrix W, we won't estimate it directly from X. Instead we are going to iterate over itself and so we will optimize W in a way. The first version of W will be initialized randomly, and over each iteration we will be *nudging* it, instead of starting from scratch each time.\n",
    "\n",
    "## Transformation in the Hidden Layer\n",
    "Let's define \"P\" as `P = W X`, so kind of how we defined \"T\" for our perceptron. This will model the hidden layer, but we are now going to add a bit more complexity. We will apply a component-wise function \"sigma\" to `P` to get `S`, so $P=WX$ and $S = sigma(P)$ \n",
    "\n",
    "We will call this \"sigma\" function an \"activation function\". This will helps us to model complex non-linear relationships and interaction effects. We will use the sigmoid activation function, which given an input \"z\" is defined as:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\n",
    "$$\n",
    "After applying sigma, its output will become the input for the next layer, which in our case will be just the output layer.\n",
    "$$ S = sigma(WX) $$\n",
    "\n",
    "We could also apply another activation function to the output layer, but for simplicity won't. \n",
    "\n",
    "\n",
    "## Output Matrix and Error Calculation\n",
    "\n",
    "Finally, our output matrix could be defined as our perceptron last time:\n",
    "$$\n",
    "Y = WX\n",
    "$$\n",
    "\n",
    "But now the input matrix X is actually the output of the hidden layer, and given that both layers have different weight matrixes let's add a subscript to each to differentiate them (\"o\" for output and \"h\" for hidden), so:\n",
    "\n",
    "$$\n",
    "Y = W_o * sigma(W_h * X)\n",
    "$$\n",
    "\n",
    "\n",
    "Like last time, we will try to make the predicted labels to be as close as posible to the real labels. We will now introduce an error function, that will be defined as:\n",
    "\n",
    "$$E(W_h, W_o) = \\sum ||t_k - y_k||^2$$\n",
    "\n",
    "Where t is a label and y a prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
