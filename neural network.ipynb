{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Basics\n",
    "\n",
    "## Neural Network Architecture: \n",
    "Understand the basic components, such as input layer, hidden layers, output layer, neurons, weights, and biases.\n",
    "## Activation Functions: \n",
    "Learn about functions like Sigmoid, ReLU, and Softmax.\n",
    "## Loss Functions: \n",
    "Understand concepts of loss functions like Mean Squared Error (MSE) or Cross-Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Outline of a Neural Network Project\n",
    "\n",
    "## Introduction\n",
    "- **Objective**: Design and implement a neural network using only NumPy, focusing on the fundamental principles of machine learning and linear algebra.\n",
    "\n",
    "## 1. Neural Network Architecture\n",
    "- **Linear Algebra Perspective**: View neurons and layers as vectors and matrices.\n",
    "- **Layers**: Input, hidden, and output layers, each represented by matrices of weights and bias vectors.\n",
    "- **Neurons**: Units of computation, aggregating input through matrix operations.\n",
    "\n",
    "## 2. Mathematical Foundations\n",
    "- **Matrix Operations**: Emphasize the use of matrix multiplication, addition, and transpose operations in the network's computations.\n",
    "- **Activation Functions**: Conceptualize as vectorized non-linear transformations (e.g., Sigmoid or ReLU applied element-wise).\n",
    "\n",
    "## 3. Forward Propagation\n",
    "- **Linear Transformation**: `Z = W*X + b`, where `W` and `X` are matrices, and `b` is a bias vector.\n",
    "- **Activation**: Apply an activation function on `Z` to introduce non-linearity.\n",
    "\n",
    "## 4. Loss Function\n",
    "- **Theory**: Understand the purpose of a loss function in quantifying prediction error.\n",
    "- **Implementation**: Use a function like MSE or Cross-Entropy, represented in a linear algebra framework.\n",
    "\n",
    "## 5. Backward Propagation\n",
    "- **Gradient Computation**: Utilize the chain rule in a matrix calculus context to compute gradients of the loss with respect to weights and biases.\n",
    "- **Backpropagation Algorithm**: Conceptually, this is the reverse of forward propagation, applying the chain rule through layers.\n",
    "\n",
    "## 6. Parameter Update\n",
    "- **Gradient Descent**: Update the weights and biases by moving in the direction opposite to the gradient.\n",
    "- **Learning Rate**: Control the step size in the gradient descent.\n",
    "\n",
    "## 7. Model Integration and Training\n",
    "- **Epochs**: Iterate over the dataset multiple times.\n",
    "- **Batch Processing**: Optionally introduce the concept of mini-batch gradient descent.\n",
    "\n",
    "## 8. Evaluation and Prediction\n",
    "- **Predictive Function**: Apply the trained model to new data.\n",
    "- **Performance Metrics**: Evaluate accuracy, precision, recall, etc.\n",
    "\n",
    "## 9. Experimental Setup\n",
    "- **Dataset**: Choose an appropriate dataset for testing.\n",
    "- **Training/Testing Split**: Emphasize the importance of unbiased model evaluation.\n",
    "\n",
    "## 10. Advanced Topics (Optional)\n",
    "- **Regularization**: Techniques like L1, L2 regularization to prevent overfitting.\n",
    "- **Optimization Algorithms**: Beyond basic gradient descent, explore algorithms like Adam, RMSprop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Single Layer Neural Networks from a Statistical Perspective\n",
    "(based on \"Introduction to statistical learning\")\n",
    "\n",
    "## Conceptual Overview\n",
    "- **Purpose**: A neural network aims to build a nonlinear function `f(X)` to predict the response `Y`, using an input vector of variables `X = (X1, X2, ..., Xp)`.\n",
    "- **Distinction**: Unlike trees, boosting, and generalized additive models, neural networks have a unique structure characterized by layers and units.\n",
    "\n",
    "## Neural Network Model\n",
    "- **Input Layer**: Consists of features `X1, ..., Xp` as units.\n",
    "- **Hidden Layer**: Each input feeds into `K` hidden units (in this example, `K=5`).\n",
    "- **Model Representation**:\n",
    "  $$ f(X) = β0 + ∑_{k=1}^{K} βk g(wk0 + ∑_{j=1}^{p} wkj Xj) $$\n",
    "- **Activation Functions**: `g(z)` is a nonlinear function. Popular choices are sigmoid and ReLU (Rectified Linear Unit).\n",
    "\n",
    "## Activation Functions\n",
    "- **Sigmoid**: \n",
    "  $ g(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **ReLU**:\n",
    "  $ g(z) = max(0, z) $\n",
    "- **Efficiency**: ReLU is preferred for its computational efficiency.\n",
    "\n",
    "## Computation in Hidden Layer\n",
    "- **Activations**: Calculated as $Ak = hk(X) = g(wk0 + ∑_{j=1}^{p} wkj Xj)$.\n",
    "- **Role of Activations**: Similar to basis functions, transforming original features.\n",
    "\n",
    "## Output Layer\n",
    "- **Formulation**: The output is a linear regression in the activations `Ak`.\n",
    "- **Parameters**: Includes both weights `w` and biases `β`, to be estimated from data.\n",
    "\n",
    "## Nonlinearity and Interaction Effects\n",
    "- Nonlinear activation functions allow the model to capture complex patterns and interactions.\n",
    "- Example with quadratic `g(z)` illustrates how nonlinear transformations can model interactions.\n",
    "\n",
    "## Fitting the Neural Network\n",
    "- **Loss Function**: Typically squared-error loss for quantitative responses.\n",
    "- **Optimization**: Minimize the sum of squared errors between predictions and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section is based on the paper \"The Neural Network, its Techniques and Applications\" by Casey Schafer (2016)*\n",
    "\n",
    "# Basic Neural Network: Perceptron Explanation\n",
    "\n",
    "A **perceptron** is the most basic form of a neural network. It consists of one layer of inputs and one layer of outputs. Let's break down the components and the operation of this network step by step, exemplifying with :\n",
    "\n",
    "## Layers and Vectors\n",
    "\n",
    "1. **Layers**: In a perceptron, layers are visualized as a series of nodes aligned vertically.\n",
    "   - An **input layer** is represented by a vector for one observation. For example, vector `x` is `[x1 x2]ᵀ`.\n",
    "   - With multiple observations, each layer is represented as a matrix.\n",
    "\n",
    "2. **Input Layer (X)**: For a perceptron with two inputs and `p` observations, the input layer is a `2 × p` matrix, denoted as `X`.\n",
    "\n",
    "3. **Output Layer (Y)**: \n",
    "   - The output is represented by the variable `y`.\n",
    "   - The output matrix `Y` is typically `n × p` but, in this basic case, it simplifies to `1 × p`.\n",
    "   - `Y` is the predicted output by the network, different from `T`, which is the known output in supervised learning.\n",
    "\n",
    "## Goal of the Neural Network\n",
    "\n",
    "- The objective is to minimize the difference between the known output `T` and the predicted output `Y`.\n",
    "- The ideal network predicts `T` accurately using only the inputs.\n",
    "\n",
    "## Weight Matrix (W)\n",
    "\n",
    "- To map from `X` to `Y`, we introduce a weight matrix `W`.\n",
    "- In general, `W` is an `n × m` matrix, where `m` is the number of input nodes, independent of the number of observations.\n",
    "- The fundamental equation of the network is: `T = W * X + b`, where `b` is a bias vector.\n",
    "\n",
    "## Linearization\n",
    "\n",
    "- To linearize this equation, augment `W` with `b` as `[W | b]` and add a row of 1’s to `X`.\n",
    "- The equation becomes: `T = [W | b] * [X; 1]`.\n",
    "- We redefine `W` to be `n × (m + 1)` and `X` to be `(m + 1) × p` with a row of 1’s at the bottom.\n",
    "\n",
    "## Solving for W\n",
    "\n",
    "- We don't \"solve\" this equation in the traditional sense. Instead, we compute the pseudoinverse of `X` to find an approximation of `W`, denoted as `Ŵ`.\n",
    "- `Ŵ` is calculated as `Ŵ = T * V * Σ⁻¹ * Uᵀ`, where `V`, `Σ`, and `U` are derived from the singular value decomposition of `X`.\n",
    "- This `Ŵ` is a projection onto the column space of `X`.\n",
    "\n",
    "## Predicted Output (Y)\n",
    "\n",
    "- The predicted output `Y` is computed as `Y = Ŵ * X`.\n",
    "- `Y` equals `T` only if `T` is in the column space of `X`, which is rare.\n",
    "- Otherwise, `Y` is as close to `T` as possible, given the constraints.\n",
    "\n",
    "## Limitations and Extensions\n",
    "\n",
    "- This linear algebra method works best if the relationship between `X` and `T` is linear, which is also rare.\n",
    "- For more complex relationships, we extend the model to a more sophisticated neural network that can handle non-linear functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "images = data['images'] # a list of 1000 numpy image matrices\n",
    "labels = data['labels'] # a list of 1000 integer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4, 5)\n",
      "T shape: (1, 5)\n",
      "X: [[-0.82564301  0.78477748  0.31311514 -1.0836756  -2.19473362]\n",
      " [ 0.71074089 -0.76406765  0.32480617  1.88997109  1.19662102]\n",
      " [ 1.40289635  0.85435855  0.64118954 -0.08368818 -0.92360596]\n",
      " [-0.43496685 -1.83350986  0.0928096  -1.55390604 -0.69541564]]\n",
      "T: [[-0.31068405 -1.01206771  0.72946336  0.8478648  -0.3655965 ]]\n",
      "W_hat shape: (1, 4)\n",
      "W_hat: [[ 0.84864578  1.22048007 -0.20432708  0.32805125]]\n",
      "New X shape: (4, 5)\n",
      "New X: [[-0.62371697  0.02019627 -0.26861686 -0.38929488  1.33407436]\n",
      " [ 1.29733104  0.29565742  0.25977682 -0.95211057  0.66521986]\n",
      " [-0.01698883 -1.25691953 -0.97536636  0.26575612 -0.84163206]\n",
      " [-0.78828163  0.39129256 -0.03291746  0.02240144 -0.40793631]]\n",
      "Predictions: [[ 0.7989264   0.76317017  0.27758701 -1.53935778  1.98218836]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_weights(n_inputs, n_outputs):\n",
    "    \"\"\"\n",
    "    Randomly initializes the weight matrix and bias vector.\n",
    "    \"\"\"\n",
    "    W = np.random.randn(n_outputs, n_inputs)\n",
    "    b = np.random.randn(n_outputs, 1)\n",
    "    return W, b\n",
    "\n",
    "def linear_transform(X, W, b):\n",
    "    \"\"\"\n",
    "    Computes the linear transformation W * X + b.\n",
    "    \"\"\"\n",
    "    return np.dot(W, X) + b\n",
    "\n",
    "def compute_pseudoinverse(X):\n",
    "    \"\"\"\n",
    "    Computes the pseudoinverse of matrix X.\n",
    "    \"\"\"\n",
    "    U, S, VT = np.linalg.svd(X, full_matrices=False)\n",
    "    S_inv = np.diag(1 / S)\n",
    "    X_pinv = np.dot(VT.T, np.dot(S_inv, U.T))\n",
    "    return X_pinv\n",
    "\n",
    "def train_perceptron(X, T):\n",
    "    \"\"\"\n",
    "    Trains the perceptron using pseudoinverse to find the optimal weights.\n",
    "    \"\"\"\n",
    "    X_pinv = compute_pseudoinverse(X)\n",
    "    W_hat = np.dot(T, X_pinv)\n",
    "    return W_hat\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Uses the trained perceptron to predict outputs.\n",
    "    \"\"\"\n",
    "    return np.dot(W, X)\n",
    "\n",
    "# Example usage\n",
    "n_inputs = 4  # Number of input nodes\n",
    "n_outputs = 1 # Number of output nodes\n",
    "p = 5         # Number of observations\n",
    "\n",
    "# Random example data\n",
    "X = np.random.randn(n_inputs, p)  # Input data\n",
    "T = np.random.randn(n_outputs, p) # Target output\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"T shape:\", T.shape)\n",
    "print(\"X:\", X)\n",
    "print(\"T:\", T)\n",
    "\n",
    "# Training the perceptron\n",
    "W_hat = train_perceptron(X, T)\n",
    "print(\"W_hat shape:\", W_hat.shape)\n",
    "print(\"W_hat:\", W_hat)\n",
    "\n",
    "# Predicting new inputs\n",
    "new_X = np.random.randn(n_inputs, p)\n",
    "\n",
    "print(\"New X shape:\", new_X.shape)\n",
    "print(\"New X:\", new_X)\n",
    "\n",
    "\n",
    "predictions = predict(new_X, W_hat)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
